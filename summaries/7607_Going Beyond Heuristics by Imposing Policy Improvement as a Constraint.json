{
  "phenomenon": "在强化学习（RL）应用中，结合启发式奖励与任务奖励对于实现理想性能至关重要。启发式编码了人类关于任务应如何完成的先验知识，为RL算法提供了有价值的提示。然而，这些提示可能不是最优的，限制了学习策略的性能。",
  "problem": [
    "(1) 启发式奖励可能限制RL算法的性能，因为它们基于人类假设，可能不是最优的。",
    "(2) 在有限数据情况下，确保基于启发式奖励的策略优于仅基于任务奖励的策略需要仔细调整启发式奖励与任务奖励之间的平衡系数λ。",
    "(3) 现有的方法（如最优策略不变性）在理论上确保无限数据下策略优于启发式策略，但在实践中往往无法在复杂机器人任务中实现。"
  ],
  "mechanism": [
    "(1) 提出了一种新原则，针对有限数据设置，训练一个结合任务和启发式奖励的策略，并确保其优于仅基于启发式奖励的策略。",
    "(2) 通过引入约束优化目标，防止策略仅利用启发式奖励而不改进任务奖励。",
    "(3) 使用拉格朗日乘子法将约束优化问题转化为无约束的最小-最大优化问题，通过梯度下降-上升策略交替优化策略和乘子。"
  ],
  "result": {
    "datasets": [
      "IsaacGym模拟器",
      "Bidexterous Manipulation (BI-DEX)基准"
    ],
    "performance": "在机器人运动、直升机控制和操作任务上的实验表明，该方法始终优于启发式策略，无论启发式奖励的质量如何。具体性能数值未在摘要中详细说明。"
  }
}