{
  "phenomenon": "在预训练阶段对大型语言模型进行所谓“原生对齐”，从一开始就避免未对齐内容，而不是事后修补。",
  "problem": {
    "1": "难以收集高质量的对齐数据。",
    "2": "对齐过程缺乏稳定性。",
    "3": "预训练数据中存在攻击性和有害内容，导致模型产生有毒输出。"
  },
  "mechanism": {
    "1": "在预训练阶段引入“原生对齐”，提升预训练模型的有效性和可用性。",
    "2": "使用以阿拉伯语及其文化为核心的数据驱动对齐方法，包括重写数据并训练较小的LLM以标注的对齐数据对。",
    "3": "通过全面实验和消融研究评估原生对齐对模型性能和对齐稳定性的影响。"
  },
  "result": {
    "dataset_environment": "ArabicMMLU、EXAMS、ACV A_clean、ACV A_all 和 AraTrust 等阿拉伯语基准",
    "performance": {
      "LLaMA3-Tamed-8B": {
        "ArabicMMLU": "50.17",
        "EXAMS": "46.15",
        "ACV A_clean": "80.17",
        "ACV A_all": "78.37",
        "AraTrust": "55.94",
        "Avg": "62.14"
      },
      "LLaMA3-Tamed-70B": {
        "ArabicMMLU": "66.56",
        "EXAMS": "55.49",
        "ACV A_clean": "82.58",
        "ACV A_all": "81.36",
        "AraTrust": "63.41",
        "Avg": "69.88"
      }
    }
  }
}