{
  "phenomenon": "Adam优化算法在深度学习中的流行性及其理论上的非收敛性问题",
  "problem": [
    "(1) Adam算法在理论上不收敛，除非以问题依赖的方式选择超参数β2",
    "(2) 现有的尝试修复Adam非收敛性的方法（如AMSGrad）需要不切实际的假设，即梯度噪声是均匀有界的",
    "(3) 在梯度估计中使用无界噪声（如高斯噪声）时，有界噪声假设经常被违反"
  ],
  "mechanism": [
    "(1) 通过从第二矩估计中移除当前梯度，消除第二矩估计与当前梯度之间的相关性",
    "(2) 改变动量更新和第二矩估计归一化的顺序，以解决Adam式动量导致的非收敛性问题",
    "(3) 提出新的自适应梯度方法ADOPT，无需依赖特定的超参数选择和强假设，即可实现最优收敛率"
  ],
  "result": {
    "datasets": [
      "MNIST",
      "CIFAR-10",
      "ImageNet",
      "OpenWebText"
    ],
    "performance": {
      "MNIST": "ADOPT在训练和测试准确率上略优于Adam、AMSGrad和AdaShift",
      "CIFAR-10": "ADOPT在ResNet-18上的测试准确率收敛速度略快于Adam",
      "ImageNet": "ADOPT在SwinTransformer上的Top-1准确率优于AdamW和AMSGrad",
      "OpenWebText": "在小批量情况下，ADOPT能够稳定训练，而Adam在训练早期会出现损失峰值并无法收敛"
    }
  }
}