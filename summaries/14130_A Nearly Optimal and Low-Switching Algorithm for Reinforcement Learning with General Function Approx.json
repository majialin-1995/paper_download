{
  "phenomenon": "探索-利用困境在具有复杂模型类的强化学习（RL）中是一个核心挑战。",
  "problem": [
    "(1) 在有限的状态空间S和动作空间A的表格MDP中，最优遗憾界已经建立，但在许多实际应用中，状态空间S和动作空间A往往很大甚至无限，导致这些遗憾保证在许多实际应用中不可接受地大。",
    "(2) 在推荐系统等许多实际应用场景中，部署效率也是一个主要关注点。例如，部署一个新的推荐策略可能需要几周时间，而系统每分钟能够收集大量数据实施固定策略。因此，大多数在线RL算法在理论研究中所要求的在每个数据点收集后改变执行策略在计算上是低效的。",
    "(3) 在具有非线性函数类的MDP中，RL算法的样本效率已经全面研究，但部署效率也是一个主要关注点。"
  ],
  "mechanism": [
    "(1) 提出了一种新的算法，单调Q学习与上置信界（MQL-UCB），用于具有一般函数近似的RL。",
    "(2) 关键算法设计包括：1) 一种通用的确定性策略切换策略，实现了低切换成本；2) 一个单调值函数结构，精心控制函数类复杂性；3) 一种方差加权回归方案，高效利用历史轨迹数据。",
    "(3) 当K足够大时，MQL-UCB实现了eO(d√HK)的最小最大最优遗憾和eO(dH)的近最优策略切换成本，其中d是函数类的eluder维度，H是规划视野，K是episode的数量。"
  ],
  "result": {
    "dataset_environment": "在具有一般函数近似的RL环境中，",
    "performance": "MQL-UCB算法在K足够大时，实现了eO(d√HK)的最小最大最优遗憾和eO(dH)的近最优策略切换成本。"
  }
}