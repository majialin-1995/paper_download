{
  "phenomenon": "在强化学习（RL）中，Transformer模型通过将决策问题表述为序列生成，展现了卓越的优越性。Transformer-based agents通过在在线环境中提供任务上下文（如多个轨迹，称为上下文RL）实现自我改进。然而，由于Transformer中注意力的二次计算复杂性，当前的上下文RL方法随着任务视野的增加而面临巨大的计算成本。相比之下，Mamba模型以其处理长期依赖关系的高效能力而闻名，这为上下文RL解决需要长期记忆的任务提供了机会。",
  "problem": [
    "(1) Transformer中自注意力机制的二次复杂性",
    "(2) 由跨情景上下文引起的长期序列的乘法增长"
  ],
  "mechanism": [
    "(1) 通过用Mamba骨干替换Decision Transformer（DT）的骨干，首先实现了Decision Mamba（DM）",
    "(2) 提出了Decision Mamba-Hybrid（DM-H），结合了Transformer和Mamba在高质量预测和长期记忆方面的优点"
  ],
  "result": {
    "datasets": [
      "D4RL",
      "Grid World",
      "Tmaze benchmarks"
    ],
    "performance": {
      "D4RL": {
        "HalfCheetah": {
          "Med-Expert": "96.21±0.28",
          "Medium": "45.45±0.35",
          "Med-Replay": "45.26±0.43"
        },
        "Hopper": {
          "Med-Expert": "117.19±0.65",
          "Medium": "83.15±0.63",
          "Med-Replay": "98.36±0.51"
        },
        "Walker2d": {
          "Med-Expert": "118.21±0.56",
          "Medium": "88.29±0.76",
          "Med-Replay": "95.66±1.16"
        }
      },
      "Grid World": "在长期任务中，DM-H的在线测试比基于Transformer的基线快28倍",
      "Tmaze": "DM-H在任何任务视野下都能以最小的在线测试成本实现最大奖励"
    }
  }
}