{
  "phenomenon": "联邦离线强化学习（Federated Offline Reinforcement Learning, FEDORA）",
  "problem": [
    "(1) 集合异质性：异质客户端数据集将生成不同性能水平的策略。",
    "(2) 悲观值计算：离线RL采用悲观方法计算数据中表现不佳的动作的值，以最小化分布偏移。",
    "(3) 数据异质性：与其他联邦学习一样，每个客户端在联邦轮次之间基于异质数据的多个局部梯度步骤可能导致有偏模型。"
  ],
  "mechanism": [
    "(1) 使用集成学习方法提炼客户端的集体智慧。",
    "(2) 通过最大化熵原则选择权重，以最佳代表客户端策略的相对优点。",
    "(3) 通过相对于联邦策略和本地数据集的正则化客户端策略来解决数据异质性。"
  ],
  "result": {
    "datasets": [
      "MuJoCo环境",
      "真实世界数据集（如TurtleBot机器人）"
    ],
    "performance": "FEDORA在各种复杂的连续控制环境和真实世界数据集中显著优于其他方法，包括在组合数据池上的离线RL。"
  }
}