{
  "phenomenon": "大型语言模型（LLMs）通过人类反馈的强化学习（RLHF）进行训练时，学习到的反馈模式（Learned Feedback Patterns, LFP）",
  "problem": [
    "(1) 不清楚LLMs是否准确学习了人类反馈数据中的潜在偏好。",
    "(2) 特征叠加（feature superposition）在密集、高维的激活空间中，以及模型的可解释性有限，模糊了人类可解释特征与模型输出之间的关系。",
    "(3) 部署通过RLHF微调的LLMs可能会放大与人类反馈数据偏好不一致的LFPs的影响，带来潜在风险。"
  ],
  "mechanism": [
    "(1) 提出LFP概念，指在RLHF过程中学习到的、能提高微调任务表现的LLM激活模式。",
    "(2) 使用探针（probes）估计微调LLM激活中隐含的反馈信号，比较这些估计与真实反馈，测量LFPs与微调反馈的准确性。",
    "(3) 探针训练基于LLM激活的稀疏、可解释表示，便于将输入特征与探针预测相关联。"
  ],
  "result": {
    "dataset_environment": [
      "IMDb数据集用于控制情感生成任务",
      "Anthropic HH-RLHF数据集用于帮助和无害性任务",
      "toxic-dpo数据集用于毒性任务"
    ],
    "performance": [
      "在控制情感生成任务中，Pythia-160m的探针预测与VADER词典之间的Kendall Tau相关系数显著（p值=0.014）。",
      "在帮助和无害性及毒性任务中，逻辑回归探针的准确率≥99.80%。",
      "通过GPT-4验证，探针识别的特征与LFPs相关的特征描述一致。"
    ]
  }
}