{
  "phenomenon": "利用大型语言模型（LLM）的知识通过离线强化学习（RL）训练知识型代理",
  "problem": [
    "(1) RL训练代理使用交互数据，限制了其能力范围；",
    "(2) LLM基于文本，与环境的数值数据存在语义鸿沟；",
    "(3) 需要将LLM的知识有效整合到RL中，以支持低级别控制和适应新情况。"
  ],
  "mechanism": [
    "(1) KALM方法通过调整LLM的架构以处理环境状态和动作；",
    "(2) 通过监督微调（SFT）使LLM理解环境数据；",
    "(3) 使用LLM生成虚构的rollouts，代理通过离线RL学习这些rollouts。"
  ],
  "result": {
    "dataset": "CLEVR-Robot和Meta-world",
    "performance": "在CLEVR-Robot模拟环境中，KALM在完成1400个不同新任务的目标上达到了46%的成功率，显著优于基线离线RL方法的26%成功率。"
  }
}