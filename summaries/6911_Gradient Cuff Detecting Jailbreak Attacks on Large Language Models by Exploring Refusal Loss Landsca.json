{
  "phenomenon": "大型语言模型（LLMs）成为突出的生成式AI工具，用户输入查询，LLM生成答案。为了减少伤害和滥用，已经努力通过高级训练技术（如来自人类反馈的强化学习（RLHF））将这些LLMs与人类价值观对齐。然而，最近的研究强调了LLMs对旨在颠覆嵌入式安全护栏的对抗性越狱尝试的脆弱性。",
  "problem": [
    "(1) LLMs对对抗性越狱尝试的脆弱性",
    "(2) 现有防御方法要么无法抵抗所有类型的越狱攻击，要么对良性查询有显著的负面影响",
    "(3) 需要一种有效的越狱检测方法，既能显著提高LLM对恶意越狱查询的拒绝能力，又能保持模型对良性用户查询的性能"
  ],
  "mechanism": [
    "(1) 定义和调查LLMs的拒绝损失（Refusal Loss）",
    "(2) 提出一种称为Gradient Cuff的方法，利用拒绝损失景观中观察到的独特属性（包括功能值及其平滑度）来设计有效的两步检测策略",
    "(3) 通过检查输入用户查询的拒绝损失和估计损失函数的梯度范数来检测越狱提示"
  ],
  "result": {
    "datasets": [
      "LLaMA-2-7B-Chat",
      "Vicuna-7B-V1.5"
    ],
    "performance": {
      "LLaMA-2-7B-Chat": {
        "average_refusal_rate": "88.3%",
        "benign_refusal_rate": "2.2%"
      },
      "Vicuna-7B-V1.5": {
        "average_refusal_rate": "74.3%",
        "benign_refusal_rate": "3.4%"
      }
    },
    "jailbreak_attacks": [
      "GCG",
      "AutoDAN",
      "PAIR",
      "TAP",
      "Base64",
      "LRL"
    ],
    "improvement": "将LLM对这些6种越狱的平均攻击成功率（ASR）从76.7%降低到25.7%"
  }
}