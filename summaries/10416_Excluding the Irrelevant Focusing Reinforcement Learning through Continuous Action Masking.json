{
  "phenomenon": "在强化学习（RL）中，连续动作空间通常被定义为多维区间。虽然区间通常能很好地反映任务的行动边界，但由于通常较大的全局动作空间导致频繁探索无关动作，这给学习带来了挑战。然而，少量的任务知识足以识别出显著较小的状态特定相关动作集。将学习集中在这些相关动作上可以显著提高训练效率和效果。",
  "problem": [
    "(1) 全局动作空间大导致频繁探索无关动作；",
    "(2) 频繁探索无关动作可能引入不必要的成本，导致收敛缓慢，甚至阻止代理学习合适的策略；",
    "(3) 离散化连续空间可能阻止学习最优策略，且模拟现实世界系统计算成本高，开发RL代理通常需要额外的现实世界训练。"
  ],
  "mechanism": [
    "(1) 通过连续动作屏蔽方法，将动作空间精确映射到状态依赖的相关动作集，确保仅执行相关动作；",
    "(2) 引入三种连续动作屏蔽方法：生成器屏蔽、射线屏蔽和分布屏蔽；",
    "(3) 利用凸集表示相关动作集，如多面体或zonotopes，扩展连续动作屏蔽的适用性到表达性凸相关动作集。"
  ],
  "result": {
    "datasets_environments": [
      "Seeker Reach-Avoid环境",
      "2D Quadrotor环境",
      "3D Quadrotor环境",
      "Mujoco Walker2D环境"
    ],
    "performance": [
      "Seeker Reach-Avoid环境：生成器屏蔽和分布屏蔽实现最高最终奖励；",
      "2D Quadrotor环境：射线屏蔽、生成器屏蔽和动作替换实现最高奖励；",
      "3D Quadrotor环境：生成器屏蔽收敛最快，但最终奖励与射线屏蔽和动作替换相似；",
      "Walker2D环境：射线屏蔽优于生成器屏蔽，动作替换表现显著较差，PPO基线未学习有意义的策略。"
    ]
  }
}