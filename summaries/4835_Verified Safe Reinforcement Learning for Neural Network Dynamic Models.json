{
  "phenomenon": "学习可靠安全的自主控制是可信自主性的核心问题之一。然而，训练一个可以被正式验证为安全的控制器仍然是一个主要挑战。",
  "problem": [
    "(1) 在非线性神经动态系统中学习验证安全控制策略的同时最大化整体性能的挑战。",
    "(2) 随着K的增长，可微分过近似技术对于可达性验证变得松散，使得验证K超过非常小的范围变得困难。",
    "(3) 控制策略π依赖于状态，很难找到一个单一的通用控制器，能够为S0中的每个起始状态实现验证安全。"
  ],
  "mechanism": [
    "(1) 一种新颖的课程学习方案，迭代地增加验证的安全范围。",
    "(2) 利用基于梯度的学习的迭代性质来利用增量验证，重用先前验证运行的信息。",
    "(3) 学习多个依赖于初始状态的验证控制器，这一想法对于更复杂的领域尤其有价值，在这些领域中学习一个单一的通用验证安全控制器极具挑战性。"
  ],
  "result": {
    "datasets_environments": [
      "车道跟随",
      "车辆避障",
      "2D四旋翼（固定和移动障碍物）",
      "3D四旋翼"
    ],
    "performance_metrics": [
      "Verified-K：S0中可以验证K步安全的区域百分比",
      "Verified-Max：S0中所有状态可以验证为安全的最大步数",
      "Emp-k：S0中在k步内经验安全的区域百分比，k=K（我们能够验证安全的步数）和k=T（总情节长度）",
      "Avg Reward：10个情节的平均奖励，报告均值和标准差"
    ],
    "performance_values": {
      "Lane Following": {
        "Verified-80": "100.0",
        "Verified-Max": "80",
        "Emp-80": "100.0",
        "Emp-500": "100.0",
        "Avg Reward": "214 ± 5"
      },
      "Vehicle Avoidance (Moving Obstacles)": {
        "Verified-50": "100.0",
        "Verified-Max": "50",
        "Emp-50": "100.0",
        "Emp-500": "100.0",
        "Avg Reward": "401 ± 4"
      },
      "2D Quadrotor (Fixed Obstacles)": {
        "Verified-50": "100.0",
        "Verified-Max": "50",
        "Emp-50": "100.0",
        "Emp-500": "100.0",
        "Avg Reward": "401 ± 20"
      },
      "2D Quadrotor (Moving Obstacles)": {
        "Verified-50": "100.0",
        "Verified-Max": "50",
        "Emp-50": "100.0",
        "Emp-500": "100.0",
        "Avg Reward": "364 ± 4"
      },
      "3D Quadrotor (Fixed Obstacles)": {
        "Verified-15": "100.0",
        "Verified-Max": "15",
        "Emp-15": "100.0",
        "Emp-500": "100.0",
        "Avg Reward": "122 ± 14"
      }
    }
  }
}