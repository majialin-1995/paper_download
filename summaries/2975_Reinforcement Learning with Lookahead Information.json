{
  "phenomenon": "强化学习（RL）中代理在决定采取哪个动作之前观察当前状态的奖励或转移实现的现象。",
  "problem": [
    "(1) 在已知环境中，前瞻信息可以显著增加收集的奖励，但在未知环境中，现有方法无法很好地适应这些观察。",
    "(2) 在标准交互模型中，代理首先选择一个动作，然后观察其对奖励和状态动态的结果，这限制了代理只能最大化预期奖励。",
    "(3) 在许多应用中，如交易和导航，代理在选择动作之前可以获得关于动作即时结果的信息（如奖励信息或转移信息）。"
  ],
  "mechanism": [
    "(1) 设计能够有效利用前瞻信息的学习算法，通过使用奖励和转移观察的经验分布进行规划。",
    "(2) 提出动态规划（'Bellman'）方程，在原始状态空间中描述最优前瞻策略。",
    "(3) 引入两种MVP算法的变体，分别用于奖励前瞻和转移前瞻，通过使用经验分布而非估计期望来进行规划。"
  ],
  "result": {
    "dataset_environment": "表格马尔可夫决策过程（MDP）模型",
    "performance": "对于奖励前瞻，算法在K个回合后实现了紧密的遗憾界限˜O(√H³SAK)；对于转移前瞻，算法实现了˜O(√H²SK(√H + √A))的遗憾界限，与同样具有前瞻信息的更强基线相比。这些算法能够比普通RL算法收集显著更多的奖励。"
  }
}