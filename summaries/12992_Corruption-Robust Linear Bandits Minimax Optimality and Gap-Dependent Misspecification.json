{
  "phenomenon": "在强化学习中，当学习者面对被破坏的奖励时，如何有效地学习。",
  "problem": [
    "(1) 缺乏对不同对抗模型和破坏措施的整体理解；",
    "(2) 缺乏对最小最大遗憾边界的完整描述；",
    "(3) 在存在破坏的情况下，如何设计算法的保证在破坏水平上有紧密的缩放。"
  ],
  "mechanism": [
    "(1) 提供了一个统一的框架来分析这些破坏；",
    "(2) 对于随机线性强盗问题，完全描述了在强破坏和弱破坏下的最小最大遗憾之间的差距；",
    "(3) 启动了被破坏的对抗性线性强盗问题的研究，获得了与破坏水平匹配的上下界。"
  ],
  "result": {
    "dataset_environment": "线性强盗问题",
    "performance": {
      "stochastic_linear_bandits": "eO(d√T + min{dC, √dC∞}) 遗憾",
      "adversarial_linear_bandits": "eO(d√T + √dC∞) 和 eO(√d3T + dC) 遗憾分别对应弱和强破坏"
    }
  }
}