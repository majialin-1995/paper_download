{
  "phenomenon": "离线视觉强化学习（Offline Visual Reinforcement Learning）中的过拟合问题和未来奖励的高估偏差。",
  "problem": [
    "(1) 表示学习中的过拟合问题，由于从有限的高维视觉输入中提取隐藏状态。",
    "(2) 离线视觉强化学习中的价值高估问题，类似于其状态空间对应物。"
  ],
  "mechanism": [
    "(1) 利用现成的强化学习模拟器作为离线策略的“测试床”，以在线方式轻松交互。",
    "(2) 引入CoWorld，一种基于模型的强化学习方法，通过状态和奖励空间的对齐来减轻跨领域差异。",
    "(3) 通过源批评模型对目标策略进行重新评估，引入温和的正则化项到目标领域批评模型的训练目标中。"
  ],
  "result": {
    "datasets": [
      "Meta-World",
      "RoboDesk",
      "DeepMind Control Suite"
    ],
    "performance": "CoWorld在多个基准测试中大幅优于现有的强化学习方法。"
  }
}