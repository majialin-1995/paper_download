{
  "phenomenon": "多模态大型语言模型（MLLMs）中的‘幻觉’现象，即生成的响应未基于输入图像。",
  "problem": [
    "(1) 现有基于人类反馈的强化学习（RLHF）方法使用二元偏好（即优、劣）来指导MLLMs学习，但这种方法可能不足以充分学习偏好。",
    "(2) 标注多级偏好数据集既昂贵又费力，且人类或AI标注的数据集常含有显著的噪声和偏见。",
    "(3) 多级偏好学习的最优目标尚不明确，多级偏好比二元偏好引入更大的复杂性，需要有效的算法来充分利用多级偏好数据集中的知识。"
  ],
  "mechanism": [
    "(1) 提出自动化多级偏好（AMP）框架，通过减少相邻级别之间的差距和引入跨级别比较，鼓励MLLMs辨别细微差异并提供更广泛的幻觉示例比较。",
    "(2) 开发无需人类或AI标注者的自动化数据集生成流程，包括多尺寸专家生成（MEG）和增量生成（IG）策略，以及自动检查机制来进一步细化数据集。",
    "(3) 设计多级直接偏好优化（MDPO）算法，扩展传统DPO算法的能力以促进多级偏好优化，并在学习目标中加入定制惩罚项以确保稳健的多级偏好学习。",
    "(4) 提出新的幻觉基准MRHal-Bench，首次专门设计用于评估多轮对话中的幻觉。"
  ],
  "result": {
    "dataset_environment": [
      "POPE基准",
      "MMHal-Bench",
      "LLaVA-Bench",
      "MRHal-Bench"
    ],
    "performance": [
      "在POPE基准上，AMP-MEG7B和AMP-MEG13B的F1分数分别为83.4和83.4，准确率分别为83.1和82.8。",
      "在MMHal-Bench上，AMP-MEG7B和AMP-MEG13B的得分分别为3.17和3.23，幻觉率分别为0.35和0.34。",
      "在MRHal-Bench上，AMP-MEG7B和AMP-MEG13B的累积/平均得分分别为4.07/4.06和4.21/4.21，幻觉率分别为0.20/0.15和0.15/0.11。",
      "在LLaVA-Bench上，AMP-MEG7B和AMP-MEG13B在对话、详细描述和复杂问题上的得分分别为89.7、89.1、98.8和94.4、91.2、95.6。"
    ]
  }
}