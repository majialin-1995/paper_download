{
  "phenomenon": "深度强化学习（RL）算法通常将策略参数化为深度网络，输出确定性动作或建模为高斯分布的随机动作，从而限制学习到单一行为模式。同时，扩散模型作为多模态学习的强大框架出现。然而，扩散策略在在线RL中的使用受到策略似然近似难以处理以及RL方法的贪婪目标的阻碍，这容易使策略偏向单一模式。",
  "problem": [
    "(1) 策略似然近似难以处理的问题；",
    "(2) RL方法的贪婪目标容易使策略偏向单一模式的问题；",
    "(3) 学习多模态策略的挑战。"
  ],
  "mechanism": [
    "(1) 提出深度扩散策略梯度（DDiffPG），一种新颖的演员-评论家算法，学习从零开始的多模态策略，参数化为扩散模型；",
    "(2) 通过现成的无监督聚类结合基于新颖性的内在动机探索和发现多种模式；",
    "(3) 形成多模态训练批次，并利用模式特定的Q学习来减轻RL目标的固有贪婪性，确保扩散策略在所有模式上的改进；",
    "(4) 进一步允许策略以模式特定的嵌入为条件，以明确控制学习到的模式。"
  ],
  "result": {
    "datasets": [
      "AntMaze-v1",
      "AntMaze-v2",
      "AntMaze-v3",
      "AntMaze-v4",
      "Reach",
      "Peg-in-hole",
      "Drawer-close",
      "Cabinet-open"
    ],
    "performance": {
      "AntMaze-v1": "成功率为1.0，平均回合长度为75.7；",
      "AntMaze-v2": "成功率为1.0，平均回合长度为66.8；",
      "AntMaze-v3": "成功率为0.98，平均回合长度为142.5；",
      "AntMaze-v4": "成功率为1.0，平均回合长度为151.1；",
      "Reach": "成功率为1.0，平均回合长度为23.8；",
      "Peg-in-hole": "成功率为1.0，平均回合长度为5.9；",
      "Drawer-close": "成功率为1.0，平均回合长度为23.6；",
      "Cabinet-open": "成功率为1.0，平均回合长度为21.1；",
      "Randomized": "成功率为1.0，平均回合长度为162.3。"
    }
  }
}