{
  "phenomenon": "元强化学习（Meta-RL）通过学习任务适应的元先验，使得强化学习算法在数据效率和泛化能力上得到提升。",
  "problem": [
    "(1) 如何利用Meta-RL提升强化学习算法的数据效率和泛化能力。",
    "(2) 现有的Meta-RL分析只能给出任务分布上期望最优性差距的上界。",
    "(3) 需要一个双层优化框架（BO-MRL），在一次性数据采集的基础上，通过多步策略优化学习任务适应的元先验。"
  ],
  "mechanism": [
    "(1) 构建一个用于Meta-RL的双层优化框架（BO-MRL），学习任务特定策略适应的元先验。",
    "(2) 在一次性数据采集上实施多步策略优化以提升数据效率。",
    "(3) 提供任务分布上期望最优性差距的上界，从而量化模型的泛化能力。"
  ],
  "result": {
    "dataset_environment": "论文通过实证验证所推导上界的正确性，并展示所提算法相对于基线的优越性，但具体数据集/环境名称及数值未给出。"
  }
}
