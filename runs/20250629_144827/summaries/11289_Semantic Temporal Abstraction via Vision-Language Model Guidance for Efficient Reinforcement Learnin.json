{
  "phenomenon": "提取时间扩展技能可以显著提高强化学习（RL）的效率，通过将稀疏奖励的复杂决策问题分解为更简单的子任务，并实现更有效的信用分配。然而，现有的抽象方法要么以无监督的方式发现技能，这通常缺乏语义信息，导致错误或分散的技能提取结果，要么需要大量的人工干预。",
  "problem": [
    "(1) 现有抽象方法缺乏语义信息，导致技能提取结果错误或分散。",
    "(2) 需要大量人工干预。",
    "(3) 在长视野任务中，特别是离线RL中，误导性的原始选项提取可能导致价值函数学习和策略学习的次优。"
  ],
  "mechanism": [
    "(1) 利用预训练的视觉语言模型（VLMs）的广泛知识，通过重新标记每个技能，逐步引导向量量化后的潜在空间更具语义意义。",
    "(2) 这种方法，称为视觉语言模型引导的时间抽象（VanTA），促进了从离线数据中发现更具解释性和任务相关的时间分割，而无需大量的人工干预或启发式方法。",
    "(3) 通过利用VLMs中的丰富信息，我们的方法可以显著优于仅依赖有限训练数据的现有离线RL方法。"
  ],
  "result": {
    "datasets": [
      "Franka Kitchen",
      "Minigrid",
      "Crafter"
    ],
    "performance": {
      "Franka Kitchen": "69.2±8.5",
      "Minigrid": "0.92±0.03",
      "Crafter": "5.46"
    }
  }
}