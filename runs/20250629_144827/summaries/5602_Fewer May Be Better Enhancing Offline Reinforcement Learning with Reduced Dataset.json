{
  "phenomenon": "离线强化学习（Offline Reinforcement Learning, Offline RL）允许智能体从预先收集的数据集中学习，而无需与环境进行进一步的交互。",
  "problem": [
    "(1) 如何从离线数据集中选择最优子集以提升算法性能和训练效率。",
    "(2) 减少数据集大小可以揭示解决类似问题所需的最小数据要求。",
    "(3) 离线强化学习依赖于大型预收集数据集，可能导致策略学习过程中产生高计算成本。",
    "(4) 额外数据不总是能提高性能，次优数据可能加剧分布偏移问题，潜在地降低策略质量。"
  ],
  "mechanism": [
    "(1) 引入ReDOR（Reduced Datasets for Offline RL）方法，将数据集选择问题框架为梯度近似优化问题。",
    "(2) 证明广泛使用的RL中的actor-critic框架可以重新表述为子模优化目标，实现高效的子集选择。",
    "(3) 采用正交匹配追踪（Orthogonal Matching Pursuit, OMP）方法，并针对离线RL进行了几项新颖的修改。",
    "(4) 通过实验结果表明，ReDOR识别的数据子集不仅提升了算法性能，而且计算复杂度显著降低。"
  ],
  "result": {
    "dataset_environment": "D4RL基准",
    "performance": "ReDOR构建的数据子集显著提高了算法性能，且计算成本较低。"
  }
}