{
  "phenomenon": "大规模自我监督学习在文本和图像领域展示了显著的能力，但在序列决策问题中实现相同的泛化仍然是一个开放的挑战。",
  "problem": {
    "1": "在离线强化学习（RL）设置中应用这些技术通常将智能体的能力限制在数据集中找到的能力内。",
    "2": "大多数RL环境代表了一组狭窄和同质的场景，限制了训练智能体的泛化能力。"
  },
  "mechanism": {
    "1": "通过程序生成数千万个基于物理的2D任务，并使用这些任务来训练一个通用的RL智能体进行物理控制。",
    "2": "引入Kinetix：一个基于物理的RL环境的开放空间，可以表示从机器人运动、抓取到视频游戏和经典RL环境的任务，所有都在一个统一的框架内。",
    "3": "利用新颖的硬件加速物理引擎Jax2D，允许在训练期间廉价地模拟数十亿个环境步骤。"
  },
  "result": {
    "dataset": "Kinetix环境",
    "performance": "训练后的智能体在2D空间中展现出强大的物理推理能力，能够零样本解决未见的人类设计环境。此外，在感兴趣的任务上微调这个通用智能体显示出比从头训练RL智能体更强的性能。这包括解决一些标准RL训练完全失败的环境。"
  }
}