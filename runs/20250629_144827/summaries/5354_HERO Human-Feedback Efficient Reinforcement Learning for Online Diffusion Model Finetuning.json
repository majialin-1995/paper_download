{
  "phenomenon": "可控生成通过Stable Diffusion (SD)微调旨在提高保真度、安全性和与人类指导的对齐。现有的从人类反馈中学习强化学习方法通常依赖于预定义的启发式奖励函数或基于大规模数据集构建的预训练奖励模型，限制了它们在收集此类数据成本高或困难的情况下的适用性。",
  "problem": [
    "(1) 依赖于预定义的启发式奖励函数或预训练奖励模型，限制了在数据收集成本高或困难的情况下的适用性。",
    "(2) 需要大量的人类反馈实例进行训练，给人类评估者带来显著负担，并限制了个性化微调的使用。"
  ],
  "mechanism": [
    "(1) 反馈对齐表示学习：一种在线训练方法，捕捉人类反馈并提供信息丰富的学习信号进行微调。",
    "(2) 反馈引导的图像生成：涉及从SD的精炼初始化样本生成图像，使生成更快地收敛到评估者的意图。"
  ],
  "result": {
    "dataset_environment": "身体部分异常校正任务",
    "performance": "HERO在在线反馈中比现有最佳方法效率高4倍。实验显示，HERO能够有效处理推理、计数、个性化和减少NSFW内容等任务，仅需0.5K在线反馈。"
  }
}